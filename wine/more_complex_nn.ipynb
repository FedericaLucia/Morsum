{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine quality prediction with Neural Net\n",
    "\n",
    "\n",
    "Create a 3 layer linear network RELU activation functions with a cross-entropy output layer classifying the wine quality using basic numpy. This should be done without using higher-level packages. All gradients and loss functions need to be defined and explained. The functions and gradients need to be defined to calculate the feedforward and backward propagation. Basic machine learning consideration when preprocessing and handling data need to be taken in consideration. Lines of code should be commented thoroughly to show understanding.\n",
    "Dataset:â€‹https://drive.google.com/file/d/1xJmZpZvxNo7X52QSHvmP34k7nVnwBdyT/view?usp=s haring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>white</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>white</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>white</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0  white            7.0              0.27         0.36            20.7   \n",
       "1  white            6.3              0.30         0.34             1.6   \n",
       "2  white            8.1              0.28         0.40             6.9   \n",
       "3  white            7.2              0.23         0.32             8.5   \n",
       "4  white            7.2              0.23         0.32             8.5   \n",
       "\n",
       "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0      0.045                 45.0                 170.0   1.0010  3.00   \n",
       "1      0.049                 14.0                 132.0   0.9940  3.30   \n",
       "2      0.050                 30.0                  97.0   0.9951  3.26   \n",
       "3      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "4      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "\n",
       "   sulphates  alcohol quality  \n",
       "0       0.45      8.8       C  \n",
       "1       0.49      9.5       C  \n",
       "2       0.44     10.1       C  \n",
       "3       0.40      9.9       C  \n",
       "4       0.40      9.9       C  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "import numpy as np\n",
    "from random import random, seed, randrange\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#plotting and visualizing libs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#read the csv file\n",
    "wine = pd.read_csv('winequalityN.csv')\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.220172</td>\n",
       "      <td>0.323736</td>\n",
       "      <td>-0.112319</td>\n",
       "      <td>0.298421</td>\n",
       "      <td>-0.283317</td>\n",
       "      <td>-0.329747</td>\n",
       "      <td>0.459204</td>\n",
       "      <td>-0.251814</td>\n",
       "      <td>0.300380</td>\n",
       "      <td>-0.095603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>0.220172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.378061</td>\n",
       "      <td>-0.196702</td>\n",
       "      <td>0.377167</td>\n",
       "      <td>-0.353230</td>\n",
       "      <td>-0.414928</td>\n",
       "      <td>0.271193</td>\n",
       "      <td>0.260660</td>\n",
       "      <td>0.225476</td>\n",
       "      <td>-0.038248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>0.323736</td>\n",
       "      <td>-0.378061</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142486</td>\n",
       "      <td>0.039315</td>\n",
       "      <td>0.133437</td>\n",
       "      <td>0.195218</td>\n",
       "      <td>0.096320</td>\n",
       "      <td>-0.328689</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>-0.010433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>-0.112319</td>\n",
       "      <td>-0.196702</td>\n",
       "      <td>0.142486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.128902</td>\n",
       "      <td>0.403439</td>\n",
       "      <td>0.495820</td>\n",
       "      <td>0.552498</td>\n",
       "      <td>-0.267050</td>\n",
       "      <td>-0.185745</td>\n",
       "      <td>-0.359706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>0.298421</td>\n",
       "      <td>0.377167</td>\n",
       "      <td>0.039315</td>\n",
       "      <td>-0.128902</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.195042</td>\n",
       "      <td>-0.279580</td>\n",
       "      <td>0.362594</td>\n",
       "      <td>0.044806</td>\n",
       "      <td>0.395332</td>\n",
       "      <td>-0.256861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  fixed acidity  volatile acidity  citric acid  \\\n",
       "fixed acidity          1.000000          0.220172     0.323736   \n",
       "volatile acidity       0.220172          1.000000    -0.378061   \n",
       "citric acid            0.323736         -0.378061     1.000000   \n",
       "residual sugar        -0.112319         -0.196702     0.142486   \n",
       "chlorides              0.298421          0.377167     0.039315   \n",
       "\n",
       "                  residual sugar  chlorides  free sulfur dioxide  \\\n",
       "fixed acidity          -0.112319   0.298421            -0.283317   \n",
       "volatile acidity       -0.196702   0.377167            -0.353230   \n",
       "citric acid             0.142486   0.039315             0.133437   \n",
       "residual sugar          1.000000  -0.128902             0.403439   \n",
       "chlorides              -0.128902   1.000000            -0.195042   \n",
       "\n",
       "                  total sulfur dioxide   density        pH  sulphates  \\\n",
       "fixed acidity                -0.329747  0.459204 -0.251814   0.300380   \n",
       "volatile acidity             -0.414928  0.271193  0.260660   0.225476   \n",
       "citric acid                   0.195218  0.096320 -0.328689   0.057613   \n",
       "residual sugar                0.495820  0.552498 -0.267050  -0.185745   \n",
       "chlorides                    -0.279580  0.362594  0.044806   0.395332   \n",
       "\n",
       "                   alcohol  \n",
       "fixed acidity    -0.095603  \n",
       "volatile acidity -0.038248  \n",
       "citric acid      -0.010433  \n",
       "residual sugar   -0.359706  \n",
       "chlorides        -0.256861  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the correlation between columns\n",
    "corr = wine.corr(method = 'pearson')\n",
    "corr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all categorical values to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert categorical data to numeric\n",
    "def mapping(data,feature):\n",
    "    featureMap=dict()\n",
    "    count=0\n",
    "    for i in sorted(data[feature].unique(),reverse=True):\n",
    "        featureMap[i]=count\n",
    "        count=count+1\n",
    "    data[feature]=data[feature].map(featureMap)\n",
    "    return data\n",
    "\n",
    "wine=mapping(wine,\"type\")\n",
    "wine=mapping(wine,\"quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                    0\n",
      "fixed acidity           0\n",
      "volatile acidity        0\n",
      "citric acid             0\n",
      "residual sugar          0\n",
      "chlorides               0\n",
      "free sulfur dioxide     0\n",
      "total sulfur dioxide    0\n",
      "density                 0\n",
      "pH                      0\n",
      "sulphates               0\n",
      "alcohol                 0\n",
      "quality                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Remove missing values\n",
    "wine = wine.dropna()\n",
    "\n",
    "### check for null values\n",
    "null = wine.isnull().sum()\n",
    "print(null)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0     0            7.0              0.27         0.36            20.7   \n",
       "1     0            6.3              0.30         0.34             1.6   \n",
       "2     0            8.1              0.28         0.40             6.9   \n",
       "3     0            7.2              0.23         0.32             8.5   \n",
       "4     0            7.2              0.23         0.32             8.5   \n",
       "\n",
       "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0      0.045                 45.0                 170.0   1.0010  3.00   \n",
       "1      0.049                 14.0                 132.0   0.9940  3.30   \n",
       "2      0.050                 30.0                  97.0   0.9951  3.26   \n",
       "3      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "4      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "\n",
       "   sulphates  alcohol  quality  \n",
       "0       0.45      8.8        3  \n",
       "1       0.49      9.5        3  \n",
       "2       0.44     10.1        3  \n",
       "3       0.40      9.9        3  \n",
       "4       0.40      9.9        3  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check what the dataset looks like with preliminary cleaning \n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting some interesting relationships between columns values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1ddf73c8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot quality vs foxed acidity\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "sns.barplot(x = 'quality', y = 'fixed acidity', data = wine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1a4a9978>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# As expected chlorides have a visible inpact on the quality of the wines. The less the better?\n",
    "\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "sns.barplot(x = 'quality', y = 'chlorides', data = wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the dataset for Neural Nets\n",
    "wine.to_csv('wine_preprocessed.csv', header = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ActivationFunction:\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "    \n",
    "    @abstractmethod \n",
    "    def transfer(self, activation):\n",
    "       \n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transfer_derivative(self, output):\n",
    "        '''\n",
    "        Calculate the derivative of an neuron output.\n",
    "        Given an output value from a neuron, we need to calculate it's slope.\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitting:\n",
    "        \n",
    "    def cross_validation_split(self, dataset, n_folds):\n",
    "        '''\n",
    "        Split a dataset into k folds\n",
    "        '''        \n",
    "        dataset_split = list()\n",
    "        dataset_copy = list(dataset)\n",
    "        fold_size = int(len(dataset) / n_folds)\n",
    "        for i in range(n_folds):\n",
    "            fold = list()\n",
    "            while len(fold) < fold_size:\n",
    "                index = randrange(len(dataset_copy))\n",
    "                fold.append(dataset_copy.pop(index))\n",
    "            dataset_split.append(fold)\n",
    "        return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(ActivationFunction):\n",
    "\n",
    "    def transfer(self, activation):\n",
    "        '''\n",
    "        Rectified Linear Unit activation function.\n",
    "        '''\n",
    "        if(activation < 0):\n",
    "            return 0\n",
    "        elif(activation >= 0):\n",
    "            return activation\n",
    "    \n",
    "    def transfer_derivative(self, output):\n",
    "        '''\n",
    "        We are using the Rectified Linear Unit transfer function, the derivative of which can be calculated as follows:\n",
    "        \n",
    "        '''\n",
    "        if(output < 0):\n",
    "            return 0\n",
    "        elif(output >= 0):\n",
    "            return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}, {'weights': [0.2550690257394217, 0.49543508709194095, 0.4494910647887381]}, {'weights': [0.651592972722763, 0.7887233511355132, 0.0938595867742349]}, {'weights': [0.02834747652200631, 0.8357651039198697, 0.43276706790505337]}, {'weights': [0.762280082457942, 0.0021060533511106927, 0.4453871940548014]}, {'weights': [0.7215400323407826, 0.22876222127045265, 0.9452706955539223]}, {'weights': [0.9014274576114836, 0.030589983033553536, 0.0254458609934608]}, {'weights': [0.5414124727934966, 0.9391491627785106, 0.38120423768821243]}, {'weights': [0.21659939713061338, 0.4221165755827173, 0.029040787574867943]}, {'weights': [0.22169166627303505, 0.43788759365057206, 0.49581224138185065]}]\n",
      "[{'weights': [0.23308445025757263, 0.2308665415409843, 0.2187810373376886, 0.4596034657377336, 0.28978161459048557, 0.021489705265908876, 0.8375779756625729, 0.5564543226524334, 0.6422943629324456, 0.1859062658947177, 0.9925434121760651]}, {'weights': [0.8599465287952899, 0.12088995980580641, 0.3326951853601291, 0.7214844075832684, 0.7111917696952796, 0.9364405867994596, 0.4221069999614152, 0.830035693274327, 0.670305566414071, 0.3033685109329176, 0.5875806061435594]}, {'weights': [0.8824790008318577, 0.8461974184283128, 0.5052838205796004, 0.5890022579825517, 0.034525830151341586, 0.24273997354306764, 0.7974042475543028, 0.4143139993007743, 0.17300740157905092, 0.548798761388153, 0.7030407620656315]}, {'weights': [0.6744858305023272, 0.3747030205016403, 0.4389616300445631, 0.5084264882499818, 0.7784426150001458, 0.5209384176131452, 0.39325509496422606, 0.4896935204622582, 0.029574963966907064, 0.04348729035652743, 0.703382088603836]}, {'weights': [0.9831877173096739, 0.5931837303800576, 0.393599686377914, 0.17034919685568128, 0.5022385584334831, 0.9820766375385342, 0.7705231398308006, 0.5396174484497788, 0.8602897789205496, 0.23217612806301458, 0.513771663187637]}]\n",
      "[{'weights': [0.9524673882682695, 0.5777948078012031, 0.45913173191066836, 0.2692794774414212, 0.5479963094662489, 0.9571162814602269]}, {'weights': [0.005709129450392925, 0.7836552326153898, 0.8204859119254819, 0.8861795808260082, 0.7405034118331963, 0.8091399008724796]}]\n",
      "Test Forward\n",
      "[14.714362714761998, 18.122608624594374]\n",
      "Test backpropagation of error\n",
      "[{'delta': -0.004486894146018383, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'output': 0.7105668883115941}]\n",
      "[{'delta': -0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'output': 0.6213859615555266}, {'delta': 0.34263065440130236, 'weights': [0.4494910647887381, 0.651592972722763], 'output': 0.6573693455986976}]\n",
      "Test training backprop algorithm\n",
      ">epoch=0, lrate=0.500, error=2.370\n",
      "[{'delta': -16527576388647.904, 'output': 0, 'weights': [-8.94789873292513e+32, -1.159212889662605e+33, -2.6343990054819024e+32]}, {'delta': -30462.281826846123, 'output': 0, 'weights': [-2.7072628926455353e+44, -3.1663616291306206e+44, -1.1175097257821627e+44]}]\n",
      "[{'delta': 0, 'output': 0, 'weights': [14455.993723925563, -6.874389166436418e+42, -1.8223576018311416e+28]}, {'delta': 1, 'output': 0, 'weights': [-16527576388647.904, -30462.281826846123, -571471707.4749755]}]\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=1, Got=0\n",
      "Expected=1, Got=0\n",
      "Expected=1, Got=0\n",
      "Expected=1, Got=0\n",
      "Expected=1, Got=0\n"
     ]
    }
   ],
   "source": [
    "class MultilayerNnClassifier:\n",
    "    \n",
    "    def initialize_network_old(self, n_inputs, n_hidden, n_outputs):\n",
    "        '''\n",
    "        Initialize a new neural network ready for training. \n",
    "        It accepts three parameters, the number of inputs, the number of neurons \n",
    "        to have in the hidden layer and the number of outputs.\n",
    "        '''\n",
    "        network = list()\n",
    "        # hidden layer has 'n_hidden' neuron with 'n_inputs' input weights plus the bias\n",
    "        hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "        network.append(hidden_layer)\n",
    "        output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "        network.append(output_layer)\n",
    "        return network\n",
    "    \n",
    "    def initialize_network(self, n_inputs, n_hidden, n_outputs):\n",
    "        '''\n",
    "        Initialize a new neural network ready for training. \n",
    "        It accepts three parameters, the number of inputs, the hidden layers and the number of outputs.\n",
    "        '''\n",
    "        network = list()\n",
    "        h = 0\n",
    "        for hidden in n_hidden:     \n",
    "            if(h==0):       \n",
    "                # hidden layer has 'hidden' neuron with 'n_inputs' input weights plus the bias\n",
    "                hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(hidden)]\n",
    "            else:\n",
    "                # hidden layer has 'hidden' neuron with 'hidden - 1' weights plus the bias\n",
    "                hidden_layer = [{'weights':[random() for i in range(n_hidden[h-1] + 1)]} for i in range(hidden)]\n",
    "            network.append(hidden_layer)\n",
    "            h += 1\n",
    "        # output layer has 'n_outputs' neuron with 'last hidden' weights plus the bias    \n",
    "        output_layer = [{'weights':[random() for i in range(n_hidden[-1] + 1)]} for i in range(n_outputs)]\n",
    "        network.append(output_layer)\n",
    "        return network\n",
    "    \n",
    "    def activate(self, weights, inputs):\n",
    "        '''\n",
    "        Calculate neuron activation for an input is the First step of forward propagation\n",
    "        activation = sum(weight_i * input_i) + bias.\n",
    "        '''\n",
    "        activation = weights[-1]  # Bias\n",
    "        for i in range(len(weights) - 1):\n",
    "            activation += weights[i] * inputs[i]\n",
    "        return activation\n",
    "    \n",
    "    def forward_propagate(self, network, activation_function, row):\n",
    "        '''\n",
    "        Forward propagate input to a network output.\n",
    "        The function returns the outputs from the last layer also called the output layer.\n",
    "        '''\n",
    "        inputs = row\n",
    "        for layer in network:\n",
    "            new_inputs = []\n",
    "            for neuron in layer:\n",
    "                activation = self.activate(neuron['weights'], inputs)\n",
    "                neuron['output'] = activation_function.transfer(activation)\n",
    "                new_inputs.append(neuron['output'])\n",
    "            inputs = new_inputs\n",
    "        return inputs\n",
    "    \n",
    "    def backward_propagate_error(self, network, activation_function, expected):\n",
    "        '''\n",
    "        Backpropagate error and store in neurons.\n",
    "        \n",
    "        The error for a given neuron can be calculated as follows:\n",
    "        \n",
    "            error = (expected - output) * transfer_derivative(output)\n",
    "            \n",
    "        Where expected is the expected output value for the neuron, \n",
    "        output is the output value for the neuron and transfer_derivative() \n",
    "        calculates the slope of the neuron's output value.\n",
    "        \n",
    "        The error signal for a neuron in the hidden layer is calculated as:\n",
    "        \n",
    "            error = (weight_k * error_j) * transfer_derivative(output)\n",
    "            \n",
    "        Where error_j is the error signal from the jth neuron in the output layer, \n",
    "        weight_k is the weight that connects the kth neuron to the current neuron \n",
    "        and output is the output for the current neuron.\n",
    "        '''\n",
    "        for i in reversed(range(len(network))):\n",
    "            layer = network[i]\n",
    "            errors = list()\n",
    "            if i != len(network) - 1:\n",
    "                for j in range(len(layer)):\n",
    "                    error = 0.0\n",
    "                    for neuron in network[i + 1]:\n",
    "                        error += (neuron['weights'][j] * neuron['delta'])\n",
    "                    errors.append(error)\n",
    "            else:\n",
    "                for j in range(len(layer)):\n",
    "                    neuron = layer[j]\n",
    "                    errors.append(expected[j] - neuron['output'])\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                neuron['delta'] = errors[j] * activation_function.transfer_derivative(neuron['output'])\n",
    "    \n",
    "    def update_weights(self, network, row, l_rate):\n",
    "        '''\n",
    "        Updates the weights for a network given an input row of data, a learning rate \n",
    "        and assume that a forward and backward propagation have already been performed.\n",
    "        \n",
    "            weight = weight + learning_rate * error * input\n",
    "            \n",
    "        Where weight is a given weight, learning_rate is a parameter that you must specify, \n",
    "        error is the error calculated by the back-propagation procedure for the neuron and \n",
    "        input is the input value that caused the error.\n",
    "        '''\n",
    "        for i in range(len(network)):\n",
    "            inputs = row[:-1]\n",
    "            if i != 0:\n",
    "                inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "            for neuron in network[i]:\n",
    "                for j in range(len(inputs)):\n",
    "                    neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "                neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "    \n",
    "    def train_network(self, network, activation_function, train, l_rate, n_epoch, n_outputs):\n",
    "        '''\n",
    "        Train a network for a fixed number of epochs.\n",
    "        The network is updated using stochastic gradient descent.\n",
    "        '''\n",
    "        for epoch in range(n_epoch + 1):\n",
    "            sum_error = 0\n",
    "            for row in train:\n",
    "                # Calculate Loss\n",
    "                outputs = self.forward_propagate(network, activation_function, row)\n",
    "                expected = [0 for i in range(n_outputs)]\n",
    "                expected[row[-1]] = 1  # Bias\n",
    "                sum_error += sum([(expected[i] - outputs[i]) ** 2 for i in range(len(expected))])\n",
    "                self.backward_propagate_error(network, activation_function, expected)\n",
    "                self.update_weights(network, row, l_rate)\n",
    "            if (epoch % 100 == 0):    \n",
    "                print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error/float(len(train))))\n",
    "    \n",
    "    def predict(self, network, activationFunction, row):\n",
    "        '''\n",
    "        Make a prediction with a network.\n",
    "        We can use the output values themselves directly as the probability of a pattern belonging to each output class.\n",
    "        It may be more useful to turn this output back into a crisp class prediction. \n",
    "        We can do this by selecting the class value with the larger probability. \n",
    "        This is also called the arg max function.\n",
    "        '''\n",
    "        outputs = self.forward_propagate(network, activationFunction, row)\n",
    "        return outputs.index(max(outputs))\n",
    "    \n",
    "    def back_propagation(self, train, test, l_rate, n_epoch, n_hidden, activationFunction):\n",
    "        '''\n",
    "        Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "        '''\n",
    "        n_inputs = len(train[0]) - 1\n",
    "        n_outputs = len(set([row[-1] for row in train]))\n",
    "        network = self.initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "        self.train_network(network, activationFunction, train, l_rate, n_epoch, n_outputs)\n",
    "        predictions = list()\n",
    "        for row in test:\n",
    "            prediction = self.predict(network, activationFunction, row)\n",
    "            predictions.append(prediction)\n",
    "        return(predictions)\n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    \n",
    "    seed(1)\n",
    "    mlp = MultilayerNnClassifier()\n",
    "    activationFunction = ReLU()\n",
    "    network = mlp.initialize_network(2, [10,5], 2)\n",
    "    for layer in network:\n",
    "        print(layer)\n",
    "       \n",
    "    # Test forward_propagate\n",
    "    print(\"Test Forward\")\n",
    "    row = [1, 0, None]\n",
    "    output = mlp.forward_propagate(network, activationFunction, row)\n",
    "    print(output)\n",
    "    \n",
    "    # Test backward_propagate_error\n",
    "    print(\"Test backpropagation of error\")\n",
    "    network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
    "            [{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
    "    expected = [0, 1]\n",
    "    mlp.backward_propagate_error(network, activationFunction, expected)\n",
    "    for layer in network:\n",
    "        print(layer)\n",
    "      \n",
    "    # Test training backprop algorithm\n",
    "    print(\"Test training backprop algorithm\")\n",
    "    seed(1)\n",
    "    dataset = [[2.7810836, 2.550537003, 0],\n",
    "        [1.465489372, 2.362125076, 0],\n",
    "        [3.396561688, 4.400293529, 0],\n",
    "        [1.38807019, 1.850220317, 0],\n",
    "        [3.06407232, 3.005305973, 0],\n",
    "        [7.627531214, 2.759262235, 1],\n",
    "        [5.332441248, 2.088626775, 1],\n",
    "        [6.922596716, 1.77106367, 1],\n",
    "        [8.675418651, -0.242068655, 1],\n",
    "        [7.673756466, 3.508563011, 1]]\n",
    "    n_inputs = len(dataset[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in dataset]))\n",
    "    network = mlp.initialize_network(n_inputs, [2], n_outputs)\n",
    "    mlp.train_network(network, activationFunction, dataset, 0.5, 20, n_outputs)    \n",
    "    for layer in network:\n",
    "        print(layer)\n",
    "    for row in dataset:\n",
    "        prediction = mlp.predict(network, activationFunction, row)\n",
    "        print('Expected=%d, Got=%d' % (row[-1], prediction))     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationEvaluator:\n",
    "        \n",
    "    def accuracy_metric(self, actual, predicted):\n",
    "        '''\n",
    "        Calculate accuracy percentage\n",
    "        '''\n",
    "        correct = 0\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] == predicted[i]:\n",
    "                correct += 1\n",
    "        return correct / float(len(actual)) * 100.0\n",
    "    \n",
    "    def evaluate_algorithm(self, dataset, splitting, algorithm, n_folds, *args):\n",
    "        '''\n",
    "        Evaluate an algorithm using a cross validation split\n",
    "        '''\n",
    "        folds = splitting.cross_validation_split(dataset, n_folds)\n",
    "        scores = list()\n",
    "        for fold in folds:\n",
    "            train_set = list(folds)\n",
    "            train_set.remove(fold)\n",
    "            train_set = sum(train_set, [])\n",
    "            test_set = list()\n",
    "            for row in fold:\n",
    "                row_copy = list(row)\n",
    "                test_set.append(row_copy)\n",
    "                row_copy[-1] = None\n",
    "            print('>train size=%d' % (len(train_set)))\n",
    "            print('>test size=%d' % (len(test_set)))    \n",
    "            predicted = algorithm(train_set, test_set, *args)\n",
    "            actual = [row[-1] for row in fold]\n",
    "            accuracy = self.accuracy_metric(actual, predicted)\n",
    "            print('>accuracy=%.3f' % (accuracy))\n",
    "            scores.append(accuracy)\n",
    "        return scores   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from csv import reader\n",
    "\n",
    "class DataPreparation:\n",
    "     \n",
    "    def load_csv(self, filename):\n",
    "        '''\n",
    "        Load a CSV file\n",
    "        '''\n",
    "        dataset = list()\n",
    "        with open(filename, 'r') as file:\n",
    "            csv_reader = reader(file)\n",
    "            for row in csv_reader:\n",
    "                if not row:\n",
    "                    continue\n",
    "                dataset.append(row)\n",
    "        return dataset\n",
    "    def str_column_to_float(self, dataset, column):\n",
    "        '''\n",
    "        Convert string column to float\n",
    "        '''\n",
    "        for row in dataset:\n",
    "            row[column] = float(row[column].strip())\n",
    "     \n",
    "    def str_column_to_int(self, dataset, column):\n",
    "        '''\n",
    "        Convert string column to integer\n",
    "        '''\n",
    "        class_values = [row[column] for row in dataset]\n",
    "        unique = set(class_values)\n",
    "        lookup = dict()\n",
    "        for i, value in enumerate(unique):\n",
    "            lookup[value] = i\n",
    "        for row in dataset:\n",
    "            row[column] = lookup[row[column]]\n",
    "        return lookup\n",
    "    \n",
    "    def dataset_minmax(self, dataset):\n",
    "        '''\n",
    "        Find the min and max values for each column\n",
    "        '''    \n",
    "        stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "        self.stats = stats\n",
    "        return stats\n",
    "\n",
    "    def normalize_dataset_classification(self, dataset, minmax):\n",
    "        '''\n",
    "        Rescale dataset columns to the range 0-1\n",
    "        '''    \n",
    "        for row in dataset:\n",
    "            for i in range(len(row) - 1):\n",
    "                row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "    \n",
    "    def denormalize_dataset_classification(self, dataset, minmax):\n",
    "        '''\n",
    "        Rescale dataset columns to the original range\n",
    "        '''    \n",
    "        for row in dataset:\n",
    "            for i in range(len(row) - 1):\n",
    "                row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])            \n",
    "    \n",
    "    def normalize_dataset_regression(self, dataset, minmax):\n",
    "        '''\n",
    "        Rescale dataset columns to the range 0-1\n",
    "        '''    \n",
    "        for row in dataset:\n",
    "            for i in range(len(row)):\n",
    "                row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])     \n",
    "\n",
    "def classificationWineRed():\n",
    "    '''\n",
    "    Test Classification on WineRed dataset\n",
    "    '''\n",
    "        \n",
    "    seed(1)\n",
    "    \n",
    "    n_folds = 5\n",
    "    l_rate = 0.3\n",
    "    n_epoch = 1000\n",
    "    n_hidden = [10]\n",
    "    \n",
    "    mlp = MultilayerNnClassifier();\n",
    "    activationFunction = ReLU()\n",
    "    dp = DataPreparation();\n",
    "    evaluator = ClassificationEvaluator();\n",
    "    splitting = Splitting();\n",
    "        \n",
    "    # Test Backprop on Seeds dataset\n",
    "    # load and prepare data\n",
    "    filename = 'wine_preprocessed.csv'\n",
    "    \n",
    "    dataset = dp.load_csv(filename)\n",
    "    \n",
    "    for i in range(len(dataset[0]) - 1):\n",
    "        dp.str_column_to_float(dataset, i)\n",
    "    # convert class column to integers\n",
    "    dp.str_column_to_int(dataset, len(dataset[0]) - 1)\n",
    "    # normalize input variables\n",
    "    minmax = dp.dataset_minmax(dataset)\n",
    "    dp.normalize_dataset_classification(dataset, minmax)    \n",
    "    # evaluate algorithm\n",
    "    scores = evaluator.evaluate_algorithm(dataset, splitting, mlp.back_propagation, n_folds, l_rate, n_epoch, n_hidden, activationFunction)  \n",
    "    print_classification_scores(scores)  \n",
    "    \n",
    "def print_classification_scores(scores):\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))   \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    options = {\n",
    "\n",
    "           1 : classificationWineRed,\n",
    "           \n",
    "           }\n",
    "    \n",
    "    var = input(\"For training print 1.\")\n",
    "    print(\"You entered \" + str(var))\n",
    "    \n",
    "options[int(var)]()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
